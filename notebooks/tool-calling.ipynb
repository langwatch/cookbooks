{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Tool Selection\n",
    "\n",
    "In this cookbook, we demonstrate how to evaluate tool calling capabilities in LLM applications using objective metrics. Like always, we'll focus on data-driven approaches to measure and improve tool selection performance.\n",
    "\n",
    "When building AI assistants, we often need them to use external tools - searching databases, calling APIs, or processing data. But how do we know if our model is selecting the right tools at the right time? Traditional evaluation methods don't capture this well.\n",
    "\n",
    "Imagine you're building a customer service bot. A user asks \"What's my account balance?\" Your assistant needs to decide: should it query the account database, ask for authentication, or simply respond with general information? Selecting the wrong tool leads to either frustrated users (if important tools are missed) or wasted resources (if unnecessary tools are called).\n",
    "\n",
    "The key insight is that tool selection quality is distinct from text generation quality. You can have a model that writes beautiful responses but consistently fails to take appropriate actions. By measuring precision and recall of tool selection decisions, we can systematically improve how our models interact with the world around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before starting, ensure you have the following packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langwatch pydantic openai pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Start by setting up LangWatch to monitor your RAG application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langwatch\n",
    "import openai\n",
    "import getpass\n",
    "import pandas as pd\n",
    "\n",
    "# Display settings for better visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Initialize OpenAI and LangWatch\n",
    "openai.api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
    "langwatch.api_key = getpass.getpass('Enter your LangWatch API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To start evaluating, you need to do 3 things: \n",
    "\n",
    "1. Define the tools that your model can call\n",
    "2. Define an evaluation dataset of queries and corresponding expected tool calls\n",
    "3. Define a function to calculate precision and recall.\n",
    "\n",
    "Before defining our tools, let's take a look at the metrics we will be working with. In contrast to RAG, we will be using a different set of metrics for evaluating tool calling, namely precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(model_tool_call, expected_tool_call):\n",
    "    if not model_tool_call:\n",
    "        return 0.0\n",
    "\n",
    "    correct_calls = sum(1 for tool in model_tool_call if tool in expected_tool_call)\n",
    "    return round(correct_calls / len(model_tool_call), 2)\n",
    "\n",
    "def calculate_recall(model_tool_call, expected_tool_call):\n",
    "    if not expected_tool_call:\n",
    "        return 1.0\n",
    "\n",
    "    if not model_tool_call:\n",
    "        return 0.0\n",
    "\n",
    "    correct_calls = sum(1 for tool in expected_tool_call if tool in model_tool_call)\n",
    "    return round(correct_calls / len(expected_tool_call), 2)\n",
    "\n",
    "def calculate_precision_recall_for_queries(df):\n",
    "    df = df.copy()\n",
    "    df[\"precision\"] = df.apply(lambda x: calculate_precision(x[\"actual\"], x[\"expected\"]), axis=1)\n",
    "    df[\"recall\"] = df.apply(lambda x: calculate_recall(x[\"actual\"], x[\"expected\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "\n",
    "- **Precision**: The ratio of correct tool calls to total tool calls\n",
    "- **Recall**: The ratio of correct tool calls to total possible tool calls\n",
    "\n",
    "In RAG, precision was less important since we relied on the model's ability to filter out relevant documents. In tool calling, precision is very important. For example, let's say the model calls the following tools: get calendar events, create reminder, and send email about the event. If all we really cared about is that the model tells us what time an event is, we don't care about the reminder nor the email. As oppposed to RAG, the model won't filter these tools out for us (technically you could chain it with another LLM to do this for you, but this is not a standard practice). It will call them, leading to increased latency and cost. Recall is, just like standard RAG, important. If we're not calling the right tools, we might miss out on potential tools that the user needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Tools\n",
    "\n",
    "Let's start by defining our tools. When starting out, you can define a small set of 3-4 tools to evaluate. Once the evaluation framework is set in place, you can scale the number of tools to evaluate. For this application, I'll be looking at 3 tools: get calendar events, create reminder, and send email about the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def send_email(email: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email to the specified address.\n",
    "    \n",
    "    Args:\n",
    "        email: The recipient's email address\n",
    "        subject: The email subject line\n",
    "        body: The content of the email\n",
    "    \n",
    "    Returns:\n",
    "        A confirmation message\n",
    "    \"\"\"\n",
    "    print(f\"Sending email to {email} with subject: {subject}\")\n",
    "    return f\"Email sent to {email}\"\n",
    "\n",
    "def get_calendar_events(start_date: str, end_date: str) -> List[dict]:\n",
    "    \"\"\"Retrieve calendar events from specified calendars.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date for events (defaults to now)\n",
    "        end_date: End date for events (defaults to 7 days from now)\n",
    "    \n",
    "    Returns:\n",
    "        List of calendar events\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Getting events between {start_date} and {end_date}\")\n",
    "    return [{\"title\": \"Sample Event\", \"date\": start_date.isoformat()}]\n",
    "\n",
    "def create_reminder(title: str, description: str, due_date: str) -> str:\n",
    "    \"\"\"Create a new reminder.\n",
    "    \n",
    "    Args:\n",
    "        title: Title of the reminder\n",
    "        description: Detailed description of the reminder\n",
    "        due_date: When the reminder is due\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation of reminder creation\n",
    "    \"\"\"\n",
    "    print(f\"Creating reminder: {title} due on {due_date}\")\n",
    "    return f\"Reminder '{title}' created for {due_date.isoformat()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use OpenAI's API to call tools. Note that OpenAI's tools parameters expects the functions to be defined in a specific way. In the utils folder, we define a function that takes a function as input and returns a schema in the format that OpenAI expects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "from openai import AsyncOpenAI\n",
    "from helpers import func_to_schema\n",
    "\n",
    "available_tools = [func_to_schema(func) for func in [send_email, get_calendar_events, create_reminder]]\n",
    "\n",
    "# Main function to generate and execute tool calls\n",
    "async def process_user_query(query: str):\n",
    "    client = AsyncOpenAI(api_key=openai.api_key)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a helpful assistant that can call tools in response to user requests. Today's date is {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    \n",
    "    response = await client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=messages,\n",
    "        tools=available_tools,\n",
    "    )\n",
    "    \n",
    "    end_time = asyncio.get_event_loop().time()\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"time\": end_time - start_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an Eval Set\n",
    "\n",
    "Now that we have our tools defined, we can define an eval set. I'll test the model for its ability to call a single and a combination of two tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    [\"Send an email to john@example.com about the project update\", [send_email]],\n",
    "    [\"What meetings do I have scheduled for tomorrow?\", [get_calendar_events]],\n",
    "    [\"Set a reminder for my dentist appointment next week\", [create_reminder]],\n",
    "    [\"Check my calendar for next week's meetings and set reminders for each one\", [get_calendar_events, create_reminder]],\n",
    "    [\"Look up my team meeting schedule and send the agenda to all participants\", [get_calendar_events, send_email]],\n",
    "    [\"Set a reminder for the client call and send a confirmation email to the team\", [create_reminder, send_email]],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you don't need a lot of examples to begin with. The first few tests are used to set up an evaluation framework that can scale with you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected</th>\n",
       "      <th>actual</th>\n",
       "      <th>time</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Send an email to john@example.com about the project update</td>\n",
       "      <td>[send_email]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What meetings do I have scheduled for tomorrow?</td>\n",
       "      <td>[get_calendar_events]</td>\n",
       "      <td>[get_calendar_events]</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Set a reminder for my dentist appointment next week</td>\n",
       "      <td>[create_reminder]</td>\n",
       "      <td>[create_reminder]</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check my calendar for next week's meetings and set reminders for each one</td>\n",
       "      <td>[get_calendar_events, create_reminder]</td>\n",
       "      <td>[get_calendar_events]</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Look up my team meeting schedule and send the agenda to all participants</td>\n",
       "      <td>[get_calendar_events, send_email]</td>\n",
       "      <td>[get_calendar_events]</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Set a reminder for the client call and send a confirmation email to the team</td>\n",
       "      <td>[create_reminder, send_email]</td>\n",
       "      <td>[create_reminder, send_email]</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          query                                expected                         actual  time  precision  recall\n",
       "0                    Send an email to john@example.com about the project update                            [send_email]                             []  0.90        0.0     0.0\n",
       "1                               What meetings do I have scheduled for tomorrow?                   [get_calendar_events]          [get_calendar_events]  0.88        1.0     1.0\n",
       "2                           Set a reminder for my dentist appointment next week                       [create_reminder]              [create_reminder]  1.37        1.0     1.0\n",
       "3     Check my calendar for next week's meetings and set reminders for each one  [get_calendar_events, create_reminder]          [get_calendar_events]  1.06        1.0     0.5\n",
       "4      Look up my team meeting schedule and send the agenda to all participants       [get_calendar_events, send_email]          [get_calendar_events]  1.19        1.0     0.5\n",
       "5  Set a reminder for the client call and send a confirmation email to the team           [create_reminder, send_email]  [create_reminder, send_email]  1.97        1.0     1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_tool_calls(response):\n",
    "    \"\"\"Extract tool calls from the new response format\"\"\"\n",
    "    tool_calls = []\n",
    "    \n",
    "    if hasattr(response, 'output') and response.output:\n",
    "        for output_item in response.output:\n",
    "            if output_item.type == 'function_call':\n",
    "                tool_calls.append(output_item.name)\n",
    "    \n",
    "    return tool_calls\n",
    "\n",
    "coros = [process_user_query(query) for query, _ in tests]\n",
    "results = await asyncio.gather(*coros)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"query\": test_item[0],\n",
    "            \"expected\": [tool.__name__ for tool in test_item[1]],\n",
    "            \"actual\": extract_tool_calls(result[\"response\"]),\n",
    "            \"time\": round(result[\"time\"], 2),\n",
    "        }\n",
    "        for test_item, result in zip(tests, results)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = calculate_precision_recall_for_queries(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation reveals interesting patterns in the model's tool selection behavior: The model demonstrates excellent precision in tool selection - when it chooses to invoke a tool, it's typically the right one for the task. This suggests the model has a strong understanding of each tool's purpose and appropriate use cases. However, we observe lower recall scores in scenarios requiring multiple tool coordination. The model sometimes fails to recognize when a complex query necessitates multiple tools working together.\n",
    "\n",
    "Consider the query: \"Look at my team meeting schedule and send the agenda to all participants.\" This requires:\n",
    "1. Retrieving calendar information (`get_calendar_events`)\n",
    "2. Composing and sending an email (`send_email`)\n",
    "\n",
    "We should also break down recall by tool category to identify which types of tools the model handles well and where it struggles. This can guide improvements like refining tool descriptions, renaming functions for clarity, or even removing tools that aren’t adding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tool</th>\n",
       "      <th>correct_calls</th>\n",
       "      <th>expected_calls</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get_calendar_events</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>create_reminder</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>send_email</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tool  correct_calls  expected_calls  recall\n",
       "1  get_calendar_events              3               3    1.00\n",
       "2      create_reminder              2               3    0.67\n",
       "0           send_email              1               3    0.33"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_per_tool_recall(df):\n",
    "    \"\"\"Calculate recall metrics for each individual tool.\"\"\"\n",
    "    # Collect all unique tools\n",
    "    all_tools = set()\n",
    "    for tools in df[\"expected\"] + df[\"actual\"]:\n",
    "        all_tools.update(tools)\n",
    "    \n",
    "    # Initialize counters\n",
    "    correct_calls = {tool: 0 for tool in all_tools}\n",
    "    expected_calls = {tool: 0 for tool in all_tools}\n",
    "    \n",
    "    # Count when each tool should have been called vs. when it was correctly called\n",
    "    for _, row in df.iterrows():\n",
    "        expected = set(row[\"expected\"])\n",
    "        actual = set(row[\"actual\"])\n",
    "        \n",
    "        for tool in expected:\n",
    "            expected_calls[tool] += 1\n",
    "            if tool in actual:\n",
    "                correct_calls[tool] += 1\n",
    "    \n",
    "    # Build results dataframe\n",
    "    results = []\n",
    "    for tool in all_tools:\n",
    "        recall = correct_calls[tool] / expected_calls[tool] if expected_calls[tool] > 0 else 0\n",
    "        results.append({\n",
    "            \"tool\": tool,\n",
    "            \"correct_calls\": correct_calls[tool],\n",
    "            \"expected_calls\": expected_calls[tool],\n",
    "            \"recall\": recall\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"recall\", ascending=False).round(2)\n",
    "\n",
    "# Calculate per-tool recall metrics\n",
    "tool_recall_df = calculate_per_tool_recall(df)\n",
    "tool_recall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows a clear preference hierarchy, with calendar queries being handled most reliably, followed by reminders, and then emails. This suggests that:\n",
    "\n",
    "1. The `send_email` tool may need improved descriptions or examples to better match user query patterns\n",
    "2. Multi-tool coordination needs enhancement, particularly for action-oriented tools\n",
    "\n",
    "This tool-specific analysis helps us target improvements where they'll have the most impact, rather than making general changes to the entire system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this cookbook, we've demonstrated how to evaluate tool calling capabilities using objective metrics like precision and recall. By systematically analyzing tool selection performance, we've gained valuable insights into where our model excels and where it needs improvement.\n",
    "\n",
    "Our evaluation revealed that the model achieves high precision (consistently selecting appropriate tools when it does make a selection) but struggles with recall for certain tools, particularly when multiple tools need to be coordinated. The `send_email` tool showed the lowest recall (0.33), indicating it's frequently overlooked even when needed.\n",
    "\n",
    "This data-driven approach to tool evaluation offers several advantages over traditional methods:\n",
    "\n",
    "1. It provides objective metrics that can be tracked over time\n",
    "2. It identifies specific tools that need improvement rather than general system issues\n",
    "3. It highlights patterns in the model's decision-making process that might not be obvious from manual testing\n",
    "\n",
    "When building your own tool-enabled AI systems, remember that tool selection is as critical as the quality of the generated text. A model that writes beautifully but fails to take appropriate actions will ultimately disappoint users. By measuring precision and recall at both the query and tool level, you can systematically improve your system's ability to take the right actions at the right time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
