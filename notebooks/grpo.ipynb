{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Agents with GRPO\n",
    "\n",
    "In this cookbook, we'll explore how to enhance the performance of agentic systems by fine-tuning them with Generalized Reinforcement from Preference Optimization (GRPO). Specifically, we'll focus on query rewriting - a critical component in retrieval systems that transforms vague user questions into more effective search queries.\n",
    "\n",
    "What makes this approach particularly exciting is that we'll be using a smaller model - Qwen 1.7B - rather than relying on massive models like GPT-4. This demonstrates how GRPO can unlock impressive capabilities from more efficient, cost-effective models that can run locally or on modest hardware.\n",
    "\n",
    "GRPO, as implemented in DSPy, is a powerful technique that generalizes popular online reinforcement learning algorithms, enabling more effective learning from interactions. By applying GRPO to query rewriting with smaller models, we can systematically improve retrieval performance without the computational and financial costs of larger models.\n",
    "\n",
    "In this notebook, we'll walk through:\n",
    "1. Setting up a DSPy environment with the Qwen 1.7B model\n",
    "2. Creating a simple query rewriting agent for retrieval\n",
    "3. Defining a reward function based on retrieval success\n",
    "4. Fine-tuning the query rewriter with GRPO\n",
    "5. Evaluating the performance improvements\n",
    "\n",
    "By the end, you'll understand how to apply GRPO to optimize query rewriting using smaller models, achieving better performance without relying on massive models or extensive manual prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before we begin, ensure you have the necessary packages. If you're running this in an environment where `dspy` and its dependencies are not yet installed, you might need to install them. For this notebook, the key libraries are `dspy` and potentially others for data handling or specific model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dspy bm25s PyStemmer git+https://github.com/Ziems/arbor.git git+https://github.com/stanfordnlp/dspy.git@refs/pull/8171/head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "First, let's configure our environment. This involves connecting to an AI model provider. In this example, we'll set up a connection to a local Arbor server, which will act as our Reinforcement Learning (RL) server. This server handles inference and RL requests over HTTP. We'll also specify and load the Qwen3-1.7B model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.clients.lm_local_arbor import ArborProvider\n",
    "\n",
    "# Connect to local Arbor server\n",
    "port = 7453\n",
    "local_lm_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "local_lm = dspy.LM(\n",
    "    model=f\"openai/arbor:{local_lm_name}\",\n",
    "    provider=ArborProvider(),\n",
    "    temperature=0.7,\n",
    "    api_base=f\"http://localhost:{port}/v1/\",\n",
    "    api_key=\"arbor\",\n",
    ")\n",
    "\n",
    "dspy.configure(lm=local_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "With our environment configured, the next step is to load a dataset. For this example, we'll use a dataset containing questions about GPT research papers (GPT-1, GPT-2, GPT-3, GPT-4). Each example contains a query and its expected answer.\n",
    "\n",
    "DSPy works with examples in a specific format, so we'll convert our raw data into `dspy.Example` objects. Each example will have a question as input and the expected answer for evaluation. We'll split our dataset into training, validation, and test sets to properly evaluate our approach.\n",
    "\n",
    "The training set will be used to optimize our agent, the validation set to tune parameters and monitor progress, and the test set for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 100, Dev size: 50, Test size: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "ds = json.load(open(\"../data/evalset/evalset.json\"))\n",
    "document_chunks = list({doc[\"document\"] for doc in ds})  \n",
    "\n",
    "# Convert to DSPy Examples\n",
    "examples = [\n",
    "    dspy.Example(question=ex[\"query\"], answers=[ex[\"answer\"]]).with_inputs(\"question\")\n",
    "    for ex in ds\n",
    "    if ex[\"answer\"].strip()\n",
    "]\n",
    "\n",
    "# Shuffle for randomness and reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(examples)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "trainset = examples[:100]\n",
    "devset = examples[100:150]\n",
    "testset = examples[150:200]\n",
    "\n",
    "print(f\"Train size: {len(trainset)}, Dev size: {len(devset)}, Test size: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Search Functionality\n",
    "\n",
    "Before building our agent, we need to implement the search functionality that will retrieve relevant documents based on a query. In a real-world application, this might connect to a vector database or search engine.\n",
    "\n",
    "For this example, we'll create a simple search function that simulates document retrieval from our corpus of GPT research papers. The function will:\n",
    "1. Take a query string and number of results (k) as input\n",
    "2. Tokenize and embed the query\n",
    "3. Retrieve the k most relevant documents based on embedding similarity\n",
    "4. Return the list of retrieved documents\n",
    "\n",
    "This search function will be used by our agent to find information relevant to user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects           \n",
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "import Stemmer\n",
    "\n",
    "#corpus = [f\"{ex.inputs()['question']} | {ans}\" for ex in trainset for ans in ex.answers]\n",
    "corpus = document_chunks\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n",
    "retriever = bm25s.BM25(k1=0.9, b=0.4)\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "# BM25 Search Wrapper\n",
    "def search(query: str, k: int = 3):\n",
    "    tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "    results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
    "    run = {corpus[doc]: float(score) for doc, score in zip(results[0], scores[0])}\n",
    "    return list(run.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Agent\n",
    "\n",
    "Now we'll create our agent using DSPy's module system. Our agent will be a simple query rewriter that takes a user question, rewrites it to be more specific and search-friendly, and then retrieves relevant documents.\n",
    "\n",
    "The agent consists of two main components:\n",
    "1. A query rewriting module that uses Chain-of-Thought reasoning to improve the original question\n",
    "2. A document retrieval step that uses our search function to find relevant information\n",
    "\n",
    "This simple agent will serve as our baseline before optimization with GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy Module for Query Rewriting\n",
    "class QueryRewriter(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rewrite = dspy.ChainOfThought(\n",
    "            dspy.Signature(\n",
    "                \"question -> rewritten_query\",\n",
    "                \"Rewrite the vague user question into a more specific search query.\"\n",
    "            )\n",
    "        )\n",
    "        self.rewrite.set_lm(dspy.settings.lm)\n",
    "\n",
    "    def forward(self, question):\n",
    "        rewritten_query = self.rewrite(question=question).rewritten_query\n",
    "        retrieved_docs = search(rewritten_query, k=3)\n",
    "        return dspy.Prediction(rewritten_query=rewritten_query, retrieved_docs=retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Reward Function\n",
    "\n",
    "For GRPO to work effectively, we need to define a reward function that evaluates the performance of our agent. This function will determine how well the agent is doing and guide the optimization process.\n",
    "\n",
    "In our case, we'll use a simple reward function that checks if any of the retrieved documents contain the expected answer. This binary reward (0 or 1) will indicate whether the agent successfully found the information needed to answer the user's question.\n",
    "\n",
    "For this example, we'll keep it simple with a binary reward based on exact substring matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Reward Function\n",
    "def contains_answer(example, pred, trace=None):\n",
    "    docs = [doc.lower() for doc in pred.retrieved_docs]\n",
    "    answers = [ans.lower() for ans in example.answers]\n",
    "\n",
    "    def normalize(text):\n",
    "        return re.sub(r\"[^a-z0-9]\", \" \", text.lower()).split()\n",
    "\n",
    "    for answer in answers:\n",
    "        answer_tokens = set(normalize(answer))\n",
    "        for doc in docs:\n",
    "            doc_tokens = set(normalize(doc))\n",
    "            if len(answer_tokens & doc_tokens) / len(answer_tokens) > 0.75:  # 75% token overlap\n",
    "                return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# Recall Score\n",
    "def recall_score(example, pred, trace=None):\n",
    "    print(\"QUESTION:\", example.inputs())\n",
    "    print(\"ANSWERS:\", example.answers)\n",
    "    print(\"RETRIEVED:\", pred.retrieved_docs)\n",
    "    predictions = [doc.lower() for doc in pred.retrieved_docs]\n",
    "    labels = [answer.lower() for answer in example.answers]\n",
    "    if not labels:\n",
    "        return 0.0\n",
    "    hits = sum(any(label in doc for doc in predictions) for label in labels)\n",
    "    return hits / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Baseline Agent\n",
    "\n",
    "Before optimizing our agent, we need to establish a baseline performance. This will help us measure the improvement achieved through GRPO.\n",
    "\n",
    "We'll use DSPy's evaluation framework to test our agent on the validation set. The evaluation will:\n",
    "1. Run the agent on each example in the validation set\n",
    "2. Apply our reward function to measure performance\n",
    "3. Calculate the average reward across all examples\n",
    "\n",
    "This baseline score will serve as our reference point for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.00 / 50 (28.0%): 100%|██████████| 50/50 [00:00<00:00, 759.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 18:28:03 INFO dspy.evaluate.evaluate: Average Metric: 14.0 / 50 (28.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline Performance: 28.0000\n"
     ]
    }
   ],
   "source": [
    "# Baseline Eval\n",
    "program = QueryRewriter()\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=contains_answer, num_threads=4, display_progress=True)\n",
    "baseline_result = evaluate(program)\n",
    "\n",
    "print(f\"\\nBaseline Performance: {baseline_result:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with GRPO\n",
    "\n",
    "Now that we have our baseline agent and evaluation metric, we can apply GRPO to optimize the agent's performance. GRPO works by:\n",
    "\n",
    "1. Sampling multiple outputs from the agent for each input\n",
    "2. Evaluating each output using our reward function\n",
    "3. Using the rewards to update the model's parameters through reinforcement learning\n",
    "\n",
    "The key parameters for GRPO include:\n",
    "- `update_interval`: How often to update the model\n",
    "- `num_samples_per_input`: How many different outputs to generate for each input\n",
    "- `num_train_steps`: Total number of training steps\n",
    "- `beta`: Controls the trade-off between optimizing for rewards and staying close to the original model\n",
    "\n",
    "We'll configure these parameters and run the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Optimized Agent\n",
    "\n",
    "After optimizing our agent with GRPO, we need to evaluate its performance to see how much it has improved. We'll use the same evaluation framework as before, but now with our optimized agent.\n",
    "\n",
    "We'll also compare the baseline and optimized agents on a specific example to see the differences in their behavior. This will help us understand how GRPO has changed the agent's query rewriting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.grpo import GRPO\n",
    "\n",
    "# Configure GRPO parameters\n",
    "train_kwargs = {\n",
    "    \"update_interval\": 3,\n",
    "    \"per_device_train_batch_size\": 2,  # reduced from 8\n",
    "    \"gradient_accumulation_steps\": 8,  # increased to maintain effective batch size\n",
    "    \"temperature\": 0.7,\n",
    "    \"beta\": 0.04,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "    \"bf16\": True,\n",
    "    \"lr_scheduler_type\": \"constant_with_warmup\",\n",
    "    \"max_prompt_length\": 512,           # add to control token length\n",
    "    \"max_completion_length\": 128,\n",
    "    \"scale_rewards\": True,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"lora\": True,\n",
    "}\n",
    "\n",
    "# Initialize the GRPO compiler\n",
    "compiler = GRPO(\n",
    "    metric=contains_answer,\n",
    "    multitask=True,\n",
    "    num_dspy_examples_per_grpo_step=4,\n",
    "    num_samples_per_input=8,\n",
    "    exclude_demos=True,\n",
    "    num_train_steps=100,\n",
    "    num_threads=24,\n",
    "    use_train_as_val=False,\n",
    "    num_steps_for_val=10,\n",
    "    train_kwargs=train_kwargs,\n",
    "    report_train_scores=False,\n",
    ")\n",
    "\n",
    "print(\"Starting GRPO optimization. This may take some time...\")\n",
    "optimized_program = compiler.compile(student=program, trainset=trainset, valset=devset)\n",
    "print(\"Optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 50 (26.0%): 100%|██████████| 50/50 [00:00<00:00, 943.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 20:40:32 INFO dspy.evaluate.evaluate: Average Metric: 13.0 / 50 (26.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline Performance: 28.00\n",
      "Optimized Performance: 26.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the optimized program\n",
    "optimized_result = evaluate(optimized_program)\n",
    "\n",
    "print(f\"\\nBaseline Performance: {baseline_result:.2f}\")\n",
    "print(f\"Optimized Performance: {optimized_result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this cookbook, we explored how to apply GRPO to optimize an LLM-based agent for query rewriting using a compact model like Qwen 1.7B. While the baseline performance was modest (28%), the GRPO-optimized agent did not show an improvement in this short run (26%).\n",
    "\n",
    "This result highlights an important consideration: meaningful improvements with reinforcement learning methods like GRPO often require longer training durations and potentially more diverse training data. In our case, the optimization was only run for a short period (approximately 2 hours), which may not have been sufficient for the model to effectively adapt and improve.\n",
    "\n",
    "Nevertheless, the setup and methodology remain valid and promising. GRPO provides a structured way to optimize agents through preference-based feedback, and with longer training or more examples, it's likely we would observe clearer gains. The techniques demonstrated here can be extended to a variety of agent types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
